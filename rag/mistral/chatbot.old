# chatbot.py
import os
from dotenv import load_dotenv
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import FAISS
from langchain_mistralai import ChatMistralAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
FAISS_INDEX_PATH = "faiss_index"
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2" # Deve essere lo stesso usato in ingest.py

def format_docs(docs):
    return "\n\n---\n\n".join([d.page_content for d in docs])

def main():
    if not MISTRAL_API_KEY:
        print("MISTRAL_API_KEY non trovata. Assicurati sia nel file .env")
        return

    if not os.path.exists(FAISS_INDEX_PATH):
        print(f"Indice FAISS non trovato in {FAISS_INDEX_PATH}. Esegui prima ingest.py.")
        return

    # 1. Carica l'embedder e l'indice FAISS
    print("Caricamento embedder e indice FAISS...")
    embeddings_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    try:
        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embeddings_model, allow_dangerous_deserialization=True)
    except Exception as e:
        print(f"Errore nel caricamento dell'indice FAISS: {e}")
        print("Assicurati che l'indice esista e sia stato creato con lo stesso modello di embedding.")
        return
        
    retriever = vector_store.as_retriever(search_kwargs={"k": 3}) # Recupera i top 3 chunk

    # 2. Inizializza il modello Mistral
    # Usa mistral-tiny per economicità
    # Altri modelli disponibili: mistral-small, mistral-medium, mistral-large
    llm = ChatMistralAI(model_name="mistral-small", mistral_api_key=MISTRAL_API_KEY, temperature=0.1)

    # 3. Prompt Template con regole di tono e limiti
    # IMPORTANTE: Qui definisci le regole!
    prompt_template_str = """
    Sei un assistente virtuale. Il tuo compito è rispondere alle domande degli utenti basandoti ESCLUSIVAMENTE sulle informazioni fornite nel seguente contesto.
    Non devi MAI inventare informazioni o usare conoscenze esterne.
    Il tuo tono deve essere professionale, conciso e utile.
    Se le informazioni nel contesto non sono sufficienti per rispondere alla domanda, rispondi:
    "Mi dispiace, non ho trovato informazioni sufficienti nei documenti forniti per rispondere alla tua domanda."
    Non aggiungere frasi come "Spero questo aiuti" o simili. Sii diretto.
    Non fare riferimento a te stesso come "un modello linguistico" o "un'IA".

    Contesto fornito:
    {context}

    Domanda dell'utente:
    {question}

    Risposta:
    """
    prompt = ChatPromptTemplate.from_template(prompt_template_str)

    # 4. Creazione della catena RAG (Retrieval-Augmented Generation)
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    print("\nChatbot RAG inizializzato. Digita 'esci' per terminare.")
    print("----------------------------------------------------")

    while True:
        user_query = input("Tu: ")
        if user_query.lower() == 'esci':
            break
        if not user_query.strip():
            continue

        print("Bot: Sto pensando...")
        try:
            response = rag_chain.invoke(user_query)
            print(f"Bot: {response}")
        except Exception as e:
            print(f"Bot: Si è verificato un errore: {e}")
        print("----------------------------------------------------")

if __name__ == "__main__":
    main()
