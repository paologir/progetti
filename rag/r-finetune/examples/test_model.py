#!/usr/bin/env python3
"""
Test Script for the Fine-tuned R-LLM Model
Interactive testing and example usage
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from scripts.training.train_gemma_r import GemmaRTrainer


def test_model_responses():
    """Test the model with various R programming prompts"""
    
    # Initialize trainer
    model_path = "./models/r-llm-checkpoints/final_model"
    trainer = GemmaRTrainer()
    
    # Load the trained model
    try:
        trainer.setup_model()
        print("‚úÖ Model loaded successfully")
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return
    
    # Test prompts covering different R tasks
    test_prompts = [
        # Data manipulation
        "How do I filter rows in a data frame where age > 30?",
        "Write R code to group data by category and calculate mean values",
        "How can I merge two data frames in R?",
        
        # Data visualization
        "Create a histogram using ggplot2",
        "How do I make a scatter plot with a trend line in R?",
        "Write code for a bar chart showing counts by category",
        
        # Statistics
        "How do I perform a t-test in R?",
        "Calculate correlation between two variables",
        "Fit a linear regression model in R",
        
        # Data import/export
        "How do I read a CSV file in R?",
        "Write data to an Excel file using R",
        
        # Basic operations
        "Calculate mean and standard deviation of a vector",
        "How do I remove missing values from data?",
        "Create a sequence of numbers from 1 to 100"
    ]
    
    print("\nüß™ Testing R-LLM Model Responses")
    print("="*50)
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nüìù Test {i}: {prompt}")
        print("-" * 60)
        
        try:
            response = trainer.generate_sample(prompt, max_length=300)
            print(f"ü§ñ Response:\n{response}")
            
            # Basic quality check
            has_code = any(indicator in response.lower() for indicator in 
                          ['library(', '<-', 'data.frame', 'ggplot', '%>%', '```'])
            quality = "‚úÖ Contains R code" if has_code else "‚ö†Ô∏è No clear R code detected"
            print(f"\n{quality}")
            
        except Exception as e:
            print(f"‚ùå Error generating response: {e}")
        
        print("\n" + "="*60)
    
    print("\nüéØ Testing complete!")


def interactive_test():
    """Interactive testing mode"""
    
    # Initialize trainer
    trainer = GemmaRTrainer()
    
    try:
        trainer.setup_model()
        print("‚úÖ Model loaded successfully")
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return
    
    print("\nüéØ Interactive R-LLM Testing")
    print("Type your R programming questions. Type 'quit' to exit.")
    print("="*50)
    
    while True:
        try:
            prompt = input("\nüìù Your question: ").strip()
            
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("üëã Goodbye!")
                break
            
            if not prompt:
                continue
            
            print("\nü§ñ Thinking...")
            response = trainer.generate_sample(prompt, max_length=400)
            
            print(f"\nü§ñ Response:\n{response}")
            print("\n" + "-"*50)
            
        except KeyboardInterrupt:
            print("\nüëã Goodbye!")
            break
        except Exception as e:
            print(f"‚ùå Error: {e}")


def benchmark_performance():
    """Benchmark model performance on standard tasks"""
    
    trainer = GemmaRTrainer()
    
    try:
        trainer.setup_model()
        print("‚úÖ Model loaded successfully")
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return
    
    # Standard R tasks for benchmarking
    benchmark_tasks = [
        {
            "prompt": "Load a CSV file and display first 5 rows",
            "expected_keywords": ["read.csv", "head", "read_csv"]
        },
        {
            "prompt": "Create a bar plot using ggplot2",
            "expected_keywords": ["ggplot", "geom_bar", "aes"]
        },
        {
            "prompt": "Calculate summary statistics for a numeric variable",
            "expected_keywords": ["summary", "mean", "sd", "median"]
        },
        {
            "prompt": "Filter data where column value equals specific condition",
            "expected_keywords": ["filter", "subset", "==", "%>%"]
        },
        {
            "prompt": "Perform simple linear regression",
            "expected_keywords": ["lm", "~", "summary"]
        }
    ]
    
    print("\nüìä Benchmarking Model Performance")
    print("="*50)
    
    total_score = 0
    
    for i, task in enumerate(benchmark_tasks, 1):
        print(f"\nüîç Benchmark {i}: {task['prompt']}")
        
        try:
            response = trainer.generate_sample(task['prompt'], max_length=250)
            
            # Score based on keyword presence
            score = 0
            found_keywords = []
            
            for keyword in task['expected_keywords']:
                if keyword.lower() in response.lower():
                    score += 1
                    found_keywords.append(keyword)
            
            task_score = score / len(task['expected_keywords'])
            total_score += task_score
            
            print(f"üìà Score: {task_score:.2f} ({score}/{len(task['expected_keywords'])} keywords found)")
            print(f"‚úÖ Found: {', '.join(found_keywords)}")
            print(f"ü§ñ Response: {response[:200]}{'...' if len(response) > 200 else ''}")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            
        print("-" * 50)
    
    avg_score = total_score / len(benchmark_tasks)
    print(f"\nüéØ Overall Benchmark Score: {avg_score:.2f} ({avg_score*100:.1f}%)")
    
    if avg_score >= 0.8:
        print("üåü Excellent performance!")
    elif avg_score >= 0.6:
        print("üëç Good performance!")
    elif avg_score >= 0.4:
        print("‚ö†Ô∏è Moderate performance - consider more training")
    else:
        print("‚ùå Poor performance - model needs improvement")


def main():
    """Main function to run different test modes"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Test the fine-tuned R-LLM model")
    parser.add_argument("--mode", choices=["test", "interactive", "benchmark"], 
                       default="test", help="Testing mode")
    
    args = parser.parse_args()
    
    if args.mode == "test":
        test_model_responses()
    elif args.mode == "interactive":
        interactive_test()
    elif args.mode == "benchmark":
        benchmark_performance()


if __name__ == "__main__":
    main()